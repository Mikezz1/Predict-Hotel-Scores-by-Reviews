{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast, RobertaTokenizer, DistilBertTokenizer, DistilBertForSequenceClassification","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH_TO_TRAIN_DATA = '../input/hseds-texts-2020/train.csv'\nPATH_TO_TEST_DATA = '../input/hseds-texts-2020/test.csv'\n\ntrain = pd.read_csv(PATH_TO_TRAIN_DATA)\ntest = pd.read_csv(PATH_TO_TEST_DATA)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_val = train_test_split(train)\n\ntrain_text = df_train['positive'] + ' ' + df_train['negative']\nval_text = df_val['positive'] + ' ' + df_val['negative']\ntrain_labels = df_train['score']\nval_labels = df_val['score']","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading pretrained model from Hugging Face library"},{"metadata":{},"cell_type":"markdown","source":"We also may use DistillBert - more lightweight modification of Bert model"},{"metadata":{"trusted":true},"cell_type":"code","source":"bert = AutoModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c17092e0b5674d399e615b23c80af135"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"800d5d4d1711494394a8dc7bd553af34"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa72bd307ce4d289efb65c19dd456df"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b784f23f5e412f9537f25d2cfe0f88"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize sequences with Bert tokenizer and create dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_len = 200","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)","execution_count":8,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get labels, seqs and attentions masks from bert tokenizer\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32\n\n# create dataloaders\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_seq, val_mask, val_y)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Specify model "},{"metadata":{},"cell_type":"markdown","source":"We may also freeze all downloaded Bert parameters and train only classification head, however it results in much higher MAE score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# freeze parameters\n#for param in bert.parameters():\n#    param.requires_grad = False","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class bert_clf(nn.Module):\n\n    def __init__(self, bert):\n\n        super(bert_clf, self).__init__()\n\n        self.bert = bert \n        \n        # We add 3 layer fc head for classification task. It performes much better than simple linear layer\n        self.fc1 = nn.Linear(768,384)\n        self.relu =  nn.ReLU()\n        self.fc2 = nn.Linear(384,128)\n        # output dim is 1 since it is more  convinient to solve regression task than classification with\n        # 100 classes ranging from 1 to 10\n        self.fc3 = nn.Linear(128,1)\n        self.dropout = torch.nn.Dropout(0.1)\n        \n\n    def forward(self, sent_id, mask):\n\n        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n        x = self.fc1(cls_hs)\n        x = self.dropout(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        \n        return x","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and val loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n  \n    model.train()\n\n    total_loss = 0\n    total_preds=[]\n\n    for step,batch in enumerate(train_dataloader):\n        \n        batch = [i.to(device) for i in batch]\n        sent_id, mask, labels = batch\n        model.zero_grad()        \n        \n        preds = model(sent_id, mask)\n        loss = criterion(preds.flatten(), labels)\n        \n        total_loss = total_loss + loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        preds=preds.detach().cpu().numpy()\n        total_preds.append(preds)\n        \n        if step % 200 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}. Batch loss {}'.format(step, len(train_dataloader), loss))\n\n    avg_loss = total_loss / len(train_dataloader)\n\n    # (n batches, batch size, n classes) -->\n    # (n samples, n classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds\n\n\n\n\ndef evaluate():\n  \n    print(\"\\nEvaluating...\")\n    model.eval()\n    total_loss = 0\n    total_preds = []\n    for step,batch in enumerate(val_dataloader):\n        #if step % 100 == 0 and not step == 0:\n        #    print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n            \n        batch = [t.to(device) for t in batch]\n        sent_id, mask, labels = batch\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            loss = criterion(preds.flatten(),labels)\n            total_loss = total_loss + loss.item()\n            preds = preds.detach().cpu().numpy()\n            total_preds.append(preds)\n\n            \n    avg_loss = total_loss / len(val_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Proceed to training and validating"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = bert_clf(bert)\nmodel = model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 5e-5)\ncriterion = torch.nn.L1Loss()\nepochs = 3","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_valid_loss = float('inf')\n\ntrain_losses=[]\nvalid_losses=[]\n\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    train_loss, _ = train()\n    train_losses.append(train_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    \n    if (epoch % 2 ==0) or (epoch == epochs - 1):\n        valid_loss, _ = evaluate()\n    \n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'bert.pt')\n    \n        valid_losses.append(valid_loss)\n        print(f'Validation Loss: {valid_loss:.3f}')\n    \n    ","execution_count":null,"outputs":[{"output_type":"stream","text":"\n Epoch 1 / 3\n  Batch   200  of  2,344. Batch loss 0.8100377321243286\n  Batch   400  of  2,344. Batch loss 0.6973710060119629\n  Batch   600  of  2,344. Batch loss 0.8882416486740112\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load weights of best model\npath = 'bert.pt'\nmodel.load_state_dict(torch.load(path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}