{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nimport pandas as pd\nimport string\nimport nltk\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import PorterStemmer\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim import lr_scheduler\n\nnltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH_TO_TRAIN_DATA = '../input/hseds-texts-2020/train.csv'\nPATH_TO_TEST_DATA = '../input/hseds-texts-2020/test.csv'\n\ndf = pd.read_csv(PATH_TO_TRAIN_DATA)\ntest = pd.read_csv(PATH_TO_TEST_DATA)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process(text):\n    lemmatizer = WordNetLemmatizer()\n    return [word for word in word_tokenize(text.lower()) if word not in string.punctuation]\n\ndef merge(text):\n    text['all'] = text['positive'] + ' ' + text['negative']\n    text['all'] = text['all'].apply(process)\n    return text\n\ntest = merge(test)\ndf = merge(df)\n\n\ndf_train, df_test = train_test_split(df)\ny_train = df_train['score']\ny_test = df_test['score']\n\ndf_train = df_train.reset_index()\ndf_test = df_test.reset_index()\ndf_test.drop(['review_id'],inplace=True,axis=1)\ndf_train.drop(['review_id'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating vocabulary with given mininal word frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_vocab(texts, min_count=8):\n    counter = {}\n    WORDS = set()\n    WORDS.add('<UNK>')\n    for sent in list(texts):\n        for w in sent:\n            if w in counter:\n                counter[w] +=1\n            else:\n                counter[w] = 1\n            WORDS.add(w)\n\n    for i in counter.keys():\n        if counter[i] < min_count:\n            WORDS.remove(i)\n    return WORDS, len(WORDS)\n\nWORDS, len_vocab = get_vocab(df['all'], min_count=8)\nlen_vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating datasets and dataloaders with padding and truncation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_padded_data(texts, vocab = WORDS, seq_length=200):\n\n    int2word = dict(enumerate(tuple(WORDS)))\n    word2int = {w: ii for ii, w in int2word.items()}\n    pad = pad_sequence([torch.as_tensor([word2int[w] if w in WORDS else word2int['<UNK>']\n                                                   for w in seq][:seq_length]) for seq in texts], \n                               batch_first=True)\n\n    return pad\n\n\n\ntrain_pos_pad, test_pos_pad = get_padded_data(df_train['all']), get_padded_data(df_test['all'])\nsub_pos_pad = get_padded_data(test['all'])\nall_train_pos_pad = get_padded_data(df['all'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_pos_pad.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ReviewsDataset(torch.utils.data.Dataset):\n    def __init__(self, data, target=0, include_score = True):\n        self.review=data\n        self.score = target\n        self.include_score = include_score\n\n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self, idx):\n        text = self.review[idx]\n        if self.include_score:\n            score = self.score[idx]\n            return text, score\n        else:\n            return text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 750\n\ntrain_dataset = ReviewsDataset(train_pos_pad, df_train['score'], include_score = True)\ntest_dataset = ReviewsDataset(test_pos_pad, df_test['score'], include_score = True)\nall_train_dataset = ReviewsDataset(all_train_pos_pad,df['score'], include_score = True)\nsub_dataset = ReviewsDataset(sub_pos_pad, include_score = False)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\nall_train_dataloader = torch.utils.data.DataLoader(all_train_dataset, batch_size=BATCH_SIZE)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n# to predict\nsub_dataloader = torch.utils.data.DataLoader(sub_dataset, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Specifying our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GRU(nn.Module):\n    \n    def __init__(self, embedding_dim, vocab_size = len(WORDS), n_hidden=150, n_layers=2, lr=0.01,drop_prob=0.4):\n        super().__init__()\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.lr = lr\n        \n        self.gru = nn.GRU(embedding_dim, n_hidden, n_layers,\n                            batch_first=True, dropout=drop_prob)\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, scale_grad_by_freq =True)\n        self.fc = nn.Linear(n_hidden, 50)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(50, 1)\n\n        \n        \n    def  forward(self, x):\n        \n        embeds = self.word_embeddings(x)\n        embeds.permute(1, 0, 2)\n        gru_out, hidden = self.gru(embeds)\n        out = self.fc(hidden[-1])\n        out = self.fc2(self.relu(out))\n\n        return out\n    \n    def init_hidden(self, batch_size):\n\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n        \n        return hidden[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n## Building train/val loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef train(model,\n          NUM_EPOCHS,\n          optim,\n          criterion,\n          train_dataloader,\n          val_dataloader,\n          batch_size=BATCH_SIZE,\n          clip=5,\n          print_every=10,\n         ):\n    \n    if(train_on_gpu):\n        model.cuda()\n  \n    for n in range(NUM_EPOCHS):\n        model.train()\n        clear_output()\n        counter=0\n        for x,y in train_dataloader:\n            counter +=1\n            if(train_on_gpu):\n                x, y = x.cuda(), y.cuda()\n\n                \n            model.zero_grad()\n            \n            output = model(x)\n            loss = criterion(output.double().flatten(), y.flatten())\n            loss.backward()\n            \n            nn.utils.clip_grad_norm_(model.parameters(), clip)\n            optim.step()\n            \n            # validation frequency is set by print_every param\n            \n            if counter % print_every == 0:\n                val_losses = []\n                model.eval()\n                for x, y in val_dataloader:\n\n                    with torch.no_grad():\n                        inputs, targets = x, y\n                        if(train_on_gpu):\n                            inputs, targets = inputs.cuda(), targets.cuda()\n\n                        output= model(inputs.long())\n                        val_loss = criterion(output.flatten(), targets.flatten()) ### view if train_on_gpu\n\n                        val_losses.append(val_loss.item())\n\n                model.train() \n                \n    \n                print(\"Epoch: {}/{}...\".format(n+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.4f}...\".format(loss.item()),\n                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_on_gpu = torch.cuda.is_available()\n\nfrom IPython.display import clear_output\nimport numpy as np\n\nmodel= GRU(embedding_dim=100)\nmodel.to(device)\n\nlr=0.001\n\noptim = torch.optim.Adam(model.parameters(), lr=lr)\ncriterion = torch.nn.L1Loss()\n\nepochs = 6\n\ntrain(model, epochs,optim, criterion, train_dataloader, test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The state dict keys: \\n\\n\", model.state_dict().keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = {'model': GRU(100),\n              'state_dict': model.state_dict(),\n              'optimizer' : optim.state_dict()}\n\ntorch.save(checkpoint, 'checkpoint.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## & load model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    \n    model.eval()\n    \n    return model\n#model = load_checkpoint('../input/review-classification-model/checkpoint_0.65.pth').to(device)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(dataloader, model):\n    pred=[]\n    with torch.no_grad():\n        for x in dataloader:\n            x = x.cuda()\n            # move prediction to cpu to empty gpu memory\n            pred.append(model(x).cpu())\n    return torch.cat(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = predict(sub_dataloader, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = [float(i) for i in pred]\nlen(sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['review_id'] = test['review_id']\nsubmission['score'] = sub\nsubmission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission8.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}